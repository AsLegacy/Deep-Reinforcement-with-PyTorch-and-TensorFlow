{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple DQN on Keras\n",
    "On this example we will do a DQN (Without screen states) on the CartPole problem.\n",
    "<img src=\"imgs/dqn_algo.png\" alt=\"Algorithm DQN\" style=\"width: 600px;\"/>\n",
    "\n",
    "### References\n",
    "* https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55\n",
    "* https://stackoverflow.com/questions/51960225/dqn-average-reward-decrease-after-training-for-a-period-of-time\n",
    "* https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682\n",
    "* https://towardsdatascience.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f\n",
    "* https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "## Plot function\n",
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def plot_durations(durations_t):\n",
    "    plt.figure(2)\n",
    "    plt.clf()    \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Rewards')\n",
    "    plt.plot(durations_t)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:        \n",
    "        plt.plot(moving_average(durations_t,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 600              # max number of episodes to learn from\n",
    "max_steps = 200                   # max steps in an episode\n",
    "gamma = 0.99                      # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0               # exploration probability at start (Pure exploratory)\n",
    "explore_stop = 0.01               # minimum exploration probability\n",
    "decay_rate = 0.0001               # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 16                  # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.001             # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000               # memory capacity\n",
    "batch_size = 32                   # experience mini-batch size\n",
    "pretrain_length = batch_size*10   # number experiences to pretrain the memory\n",
    "\n",
    "consecutive_win_threshold = 6     # Number of Consecutive wins before stop training\n",
    "target_updates = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, env, learning_rate=0.01, state_size=4,\n",
    "                 hidden_size=10):\n",
    "        action_size = env.action_space.n\n",
    "        # state inputs to the Q-network\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Dense(hidden_size, activation='relu',\n",
    "                             input_dim=state_size))\n",
    "        self.model.add(Dense(hidden_size, activation='relu'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "\n",
    "        self.optimizer = Adam(lr=learning_rate)#, decay=1e-6)\n",
    "        self.model.compile(loss='mse', optimizer=self.optimizer)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        # Reshape states (,4) --> [1,4]\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        # Get Q-values\n",
    "        Q_values = self.model.predict(state)[0]\n",
    "        # Act greedly\n",
    "        action = np.argmax(Q_values)\n",
    "        return action\n",
    "    \n",
    "    def q_values(self, state):\n",
    "        # Reshape states (,4) --> [1,4]\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        # Get Q-values\n",
    "        Q_values = self.model.predict(state)\n",
    "        return Q_values\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.model.set_weights(weights)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "\n",
    "# Initialize DQN Network\n",
    "mainQN = QNetwork(env, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "targetQ = QNetwork(env, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "targetQ.set_weights(mainQN.get_weights()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)),\n",
    "                               size=batch_size,\n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "DONE! Episode: 1 Total reward: 24.0 Explore P: 0.9976\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "DONE! Episode: 2 Total reward: 26.0 Explore P: 0.9951\n",
      "DONE! Episode: 3 Total reward: 15.0 Explore P: 0.9936\n",
      "DONE! Episode: 4 Total reward: 15.0 Explore P: 0.9921\n",
      "DONE! Episode: 5 Total reward: 17.0 Explore P: 0.9904\n",
      "DONE! Episode: 6 Total reward: 14.0 Explore P: 0.9891\n",
      "DONE! Episode: 7 Total reward: 14.0 Explore P: 0.9877\n",
      "DONE! Episode: 8 Total reward: 43.0 Explore P: 0.9835\n",
      "DONE! Episode: 9 Total reward: 12.0 Explore P: 0.9823\n",
      "DONE! Episode: 10 Total reward: 20.0 Explore P: 0.9804\n",
      "DONE! Episode: 11 Total reward: 30.0 Explore P: 0.9775\n",
      "DONE! Episode: 12 Total reward: 39.0 Explore P: 0.9737\n",
      "DONE! Episode: 13 Total reward: 45.0 Explore P: 0.9694\n",
      "DONE! Episode: 14 Total reward: 24.0 Explore P: 0.9671\n",
      "DONE! Episode: 15 Total reward: 17.0 Explore P: 0.9655\n",
      "DONE! Episode: 16 Total reward: 19.0 Explore P: 0.9637\n",
      "DONE! Episode: 17 Total reward: 11.0 Explore P: 0.9626\n",
      "DONE! Episode: 18 Total reward: 11.0 Explore P: 0.9616\n",
      "DONE! Episode: 19 Total reward: 12.0 Explore P: 0.9604\n",
      "DONE! Episode: 20 Total reward: 25.0 Explore P: 0.9580\n",
      "DONE! Episode: 21 Total reward: 13.0 Explore P: 0.9568\n",
      "DONE! Episode: 22 Total reward: 35.0 Explore P: 0.9535\n",
      "DONE! Episode: 23 Total reward: 15.0 Explore P: 0.9521\n",
      "DONE! Episode: 24 Total reward: 37.0 Explore P: 0.9486\n",
      "DONE! Episode: 25 Total reward: 10.0 Explore P: 0.9477\n",
      "DONE! Episode: 26 Total reward: 22.0 Explore P: 0.9456\n",
      "DONE! Episode: 27 Total reward: 18.0 Explore P: 0.9439\n",
      "DONE! Episode: 28 Total reward: 13.0 Explore P: 0.9427\n",
      "DONE! Episode: 29 Total reward: 15.0 Explore P: 0.9413\n",
      "DONE! Episode: 30 Total reward: 18.0 Explore P: 0.9396\n",
      "DONE! Episode: 31 Total reward: 18.0 Explore P: 0.9380\n",
      "DONE! Episode: 32 Total reward: 30.0 Explore P: 0.9352\n",
      "DONE! Episode: 33 Total reward: 26.0 Explore P: 0.9328\n",
      "DONE! Episode: 34 Total reward: 39.0 Explore P: 0.9292\n",
      "DONE! Episode: 35 Total reward: 11.0 Explore P: 0.9282\n",
      "DONE! Episode: 36 Total reward: 33.0 Explore P: 0.9252\n",
      "DONE! Episode: 37 Total reward: 37.0 Explore P: 0.9218\n",
      "DONE! Episode: 38 Total reward: 20.0 Explore P: 0.9200\n",
      "DONE! Episode: 39 Total reward: 18.0 Explore P: 0.9183\n",
      "DONE! Episode: 40 Total reward: 30.0 Explore P: 0.9156\n",
      "DONE! Episode: 41 Total reward: 12.0 Explore P: 0.9145\n",
      "DONE! Episode: 42 Total reward: 18.0 Explore P: 0.9129\n",
      "DONE! Episode: 43 Total reward: 12.0 Explore P: 0.9118\n",
      "DONE! Episode: 44 Total reward: 18.0 Explore P: 0.9102\n",
      "DONE! Episode: 45 Total reward: 14.0 Explore P: 0.9089\n",
      "DONE! Episode: 46 Total reward: 20.0 Explore P: 0.9071\n",
      "DONE! Episode: 47 Total reward: 35.0 Explore P: 0.9040\n",
      "DONE! Episode: 48 Total reward: 24.0 Explore P: 0.9019\n",
      "DONE! Episode: 49 Total reward: 19.0 Explore P: 0.9002\n",
      "DONE! Episode: 50 Total reward: 39.0 Explore P: 0.8967\n",
      "DONE! Episode: 51 Total reward: 20.0 Explore P: 0.8949\n",
      "DONE! Episode: 52 Total reward: 10.0 Explore P: 0.8940\n",
      "DONE! Episode: 53 Total reward: 30.0 Explore P: 0.8914\n",
      "DONE! Episode: 54 Total reward: 29.0 Explore P: 0.8888\n",
      "DONE! Episode: 55 Total reward: 16.0 Explore P: 0.8874\n",
      "DONE! Episode: 56 Total reward: 24.0 Explore P: 0.8853\n",
      "DONE! Episode: 57 Total reward: 19.0 Explore P: 0.8837\n",
      "DONE! Episode: 58 Total reward: 17.0 Explore P: 0.8822\n",
      "DONE! Episode: 59 Total reward: 21.0 Explore P: 0.8804\n",
      "DONE! Episode: 60 Total reward: 41.0 Explore P: 0.8768\n",
      "DONE! Episode: 61 Total reward: 9.0 Explore P: 0.8760\n",
      "DONE! Episode: 62 Total reward: 8.0 Explore P: 0.8753\n",
      "DONE! Episode: 63 Total reward: 23.0 Explore P: 0.8733\n",
      "DONE! Episode: 64 Total reward: 32.0 Explore P: 0.8706\n",
      "DONE! Episode: 65 Total reward: 10.0 Explore P: 0.8697\n",
      "DONE! Episode: 66 Total reward: 27.0 Explore P: 0.8674\n",
      "DONE! Episode: 67 Total reward: 10.0 Explore P: 0.8665\n",
      "DONE! Episode: 68 Total reward: 15.0 Explore P: 0.8653\n",
      "DONE! Episode: 69 Total reward: 44.0 Explore P: 0.8615\n",
      "DONE! Episode: 70 Total reward: 15.0 Explore P: 0.8602\n",
      "DONE! Episode: 71 Total reward: 27.0 Explore P: 0.8579\n",
      "DONE! Episode: 72 Total reward: 11.0 Explore P: 0.8570\n",
      "DONE! Episode: 73 Total reward: 17.0 Explore P: 0.8556\n",
      "DONE! Episode: 74 Total reward: 14.0 Explore P: 0.8544\n",
      "DONE! Episode: 75 Total reward: 20.0 Explore P: 0.8527\n",
      "DONE! Episode: 76 Total reward: 34.0 Explore P: 0.8498\n",
      "DONE! Episode: 77 Total reward: 62.0 Explore P: 0.8446\n",
      "DONE! Episode: 78 Total reward: 10.0 Explore P: 0.8438\n",
      "DONE! Episode: 79 Total reward: 16.0 Explore P: 0.8425\n",
      "DONE! Episode: 80 Total reward: 22.0 Explore P: 0.8406\n",
      "DONE! Episode: 81 Total reward: 51.0 Explore P: 0.8364\n",
      "DONE! Episode: 82 Total reward: 11.0 Explore P: 0.8355\n",
      "DONE! Episode: 83 Total reward: 20.0 Explore P: 0.8339\n",
      "DONE! Episode: 84 Total reward: 13.0 Explore P: 0.8328\n",
      "DONE! Episode: 85 Total reward: 14.0 Explore P: 0.8316\n",
      "DONE! Episode: 86 Total reward: 26.0 Explore P: 0.8295\n",
      "DONE! Episode: 87 Total reward: 53.0 Explore P: 0.8252\n",
      "DONE! Episode: 88 Total reward: 18.0 Explore P: 0.8237\n",
      "DONE! Episode: 89 Total reward: 43.0 Explore P: 0.8202\n",
      "DONE! Episode: 90 Total reward: 17.0 Explore P: 0.8188\n",
      "DONE! Episode: 91 Total reward: 21.0 Explore P: 0.8171\n",
      "DONE! Episode: 92 Total reward: 9.0 Explore P: 0.8164\n",
      "DONE! Episode: 93 Total reward: 52.0 Explore P: 0.8122\n",
      "DONE! Episode: 94 Total reward: 24.0 Explore P: 0.8103\n",
      "DONE! Episode: 95 Total reward: 15.0 Explore P: 0.8091\n",
      "DONE! Episode: 96 Total reward: 33.0 Explore P: 0.8065\n",
      "DONE! Episode: 97 Total reward: 42.0 Explore P: 0.8031\n",
      "DONE! Episode: 98 Total reward: 20.0 Explore P: 0.8016\n",
      "DONE! Episode: 99 Total reward: 21.0 Explore P: 0.7999\n",
      "DONE! Episode: 100 Total reward: 28.0 Explore P: 0.7977\n",
      "DONE! Episode: 101 Total reward: 50.0 Explore P: 0.7938\n",
      "DONE! Episode: 102 Total reward: 9.0 Explore P: 0.7931\n",
      "DONE! Episode: 103 Total reward: 16.0 Explore P: 0.7918\n",
      "DONE! Episode: 104 Total reward: 26.0 Explore P: 0.7898\n",
      "DONE! Episode: 105 Total reward: 13.0 Explore P: 0.7888\n",
      "DONE! Episode: 106 Total reward: 53.0 Explore P: 0.7846\n",
      "DONE! Episode: 107 Total reward: 21.0 Explore P: 0.7830\n",
      "DONE! Episode: 108 Total reward: 54.0 Explore P: 0.7789\n",
      "DONE! Episode: 109 Total reward: 21.0 Explore P: 0.7772\n",
      "DONE! Episode: 110 Total reward: 18.0 Explore P: 0.7759\n",
      "DONE! Episode: 111 Total reward: 23.0 Explore P: 0.7741\n",
      "DONE! Episode: 112 Total reward: 33.0 Explore P: 0.7716\n",
      "DONE! Episode: 113 Total reward: 23.0 Explore P: 0.7698\n",
      "DONE! Episode: 114 Total reward: 38.0 Explore P: 0.7670\n",
      "DONE! Episode: 115 Total reward: 63.0 Explore P: 0.7622\n",
      "DONE! Episode: 116 Total reward: 15.0 Explore P: 0.7611\n",
      "DONE! Episode: 117 Total reward: 83.0 Explore P: 0.7549\n",
      "DONE! Episode: 118 Total reward: 36.0 Explore P: 0.7522\n",
      "DONE! Episode: 119 Total reward: 21.0 Explore P: 0.7506\n",
      "DONE! Episode: 120 Total reward: 44.0 Explore P: 0.7474\n",
      "DONE! Episode: 121 Total reward: 20.0 Explore P: 0.7459\n",
      "DONE! Episode: 122 Total reward: 25.0 Explore P: 0.7441\n",
      "DONE! Episode: 123 Total reward: 32.0 Explore P: 0.7417\n",
      "DONE! Episode: 124 Total reward: 33.0 Explore P: 0.7393\n",
      "DONE! Episode: 125 Total reward: 20.0 Explore P: 0.7379\n",
      "DONE! Episode: 126 Total reward: 22.0 Explore P: 0.7363\n",
      "DONE! Episode: 127 Total reward: 26.0 Explore P: 0.7344\n",
      "DONE! Episode: 128 Total reward: 44.0 Explore P: 0.7312\n",
      "DONE! Episode: 129 Total reward: 12.0 Explore P: 0.7303\n",
      "DONE! Episode: 130 Total reward: 27.0 Explore P: 0.7284\n",
      "DONE! Episode: 131 Total reward: 21.0 Explore P: 0.7269\n",
      "DONE! Episode: 132 Total reward: 11.0 Explore P: 0.7261\n",
      "DONE! Episode: 133 Total reward: 48.0 Explore P: 0.7227\n",
      "DONE! Episode: 134 Total reward: 37.0 Explore P: 0.7200\n",
      "DONE! Episode: 135 Total reward: 112.0 Explore P: 0.7121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! Episode: 136 Total reward: 76.0 Explore P: 0.7068\n",
      "DONE! Episode: 137 Total reward: 18.0 Explore P: 0.7056\n",
      "DONE! Episode: 138 Total reward: 46.0 Explore P: 0.7024\n",
      "DONE! Episode: 139 Total reward: 113.0 Explore P: 0.6946\n",
      "DONE! Episode: 140 Total reward: 84.0 Explore P: 0.6889\n",
      "DONE! Episode: 141 Total reward: 20.0 Explore P: 0.6875\n",
      "DONE! Episode: 142 Total reward: 34.0 Explore P: 0.6852\n",
      "DONE! Episode: 143 Total reward: 35.0 Explore P: 0.6828\n",
      "DONE! Episode: 144 Total reward: 17.0 Explore P: 0.6817\n",
      "DONE! Episode: 145 Total reward: 38.0 Explore P: 0.6791\n",
      "DONE! Episode: 146 Total reward: 53.0 Explore P: 0.6756\n",
      "DONE! Episode: 147 Total reward: 69.0 Explore P: 0.6710\n",
      "DONE! Episode: 148 Total reward: 79.0 Explore P: 0.6658\n",
      "DONE! Episode: 149 Total reward: 42.0 Explore P: 0.6631\n",
      "DONE! Episode: 150 Total reward: 26.0 Explore P: 0.6614\n",
      "DONE! Episode: 151 Total reward: 71.0 Explore P: 0.6568\n",
      "DONE! Episode: 152 Total reward: 13.0 Explore P: 0.6559\n",
      "DONE! Episode: 153 Total reward: 80.0 Explore P: 0.6508\n",
      "DONE! Episode: 154 Total reward: 31.0 Explore P: 0.6488\n",
      "DONE! Episode: 155 Total reward: 52.0 Explore P: 0.6455\n",
      "DONE! Episode: 156 Total reward: 38.0 Explore P: 0.6431\n",
      "DONE! Episode: 157 Total reward: 24.0 Explore P: 0.6416\n",
      "DONE! Episode: 158 Total reward: 41.0 Explore P: 0.6390\n",
      "DONE! Episode: 159 Total reward: 30.0 Explore P: 0.6371\n",
      "DONE! Episode: 160 Total reward: 89.0 Explore P: 0.6315\n",
      "DONE! Episode: 161 Total reward: 28.0 Explore P: 0.6298\n",
      "DONE! Episode: 162 Total reward: 62.0 Explore P: 0.6260\n",
      "DONE! Episode: 163 Total reward: 66.0 Explore P: 0.6219\n",
      "DONE! Episode: 164 Total reward: 22.0 Explore P: 0.6206\n",
      "DONE! Episode: 165 Total reward: 16.0 Explore P: 0.6196\n",
      "DONE! Episode: 166 Total reward: 72.0 Explore P: 0.6152\n",
      "DONE! Episode: 167 Total reward: 72.0 Explore P: 0.6109\n",
      "DONE! Episode: 168 Total reward: 53.0 Explore P: 0.6077\n",
      "DONE! Episode: 169 Total reward: 46.0 Explore P: 0.6050\n",
      "DONE! Episode: 170 Total reward: 44.0 Explore P: 0.6024\n",
      "DONE! Episode: 171 Total reward: 80.0 Explore P: 0.5976\n",
      "DONE! Episode: 172 Total reward: 92.0 Explore P: 0.5923\n",
      "DONE! Episode: 173 Total reward: 97.0 Explore P: 0.5866\n",
      "DONE! Episode: 174 Total reward: 59.0 Explore P: 0.5832\n",
      "DONE! Episode: 175 Total reward: 46.0 Explore P: 0.5806\n",
      "DONE! Episode: 176 Total reward: 70.0 Explore P: 0.5766\n",
      "DONE! Episode: 177 Total reward: 78.0 Explore P: 0.5722\n",
      "DONE! Episode: 178 Total reward: 66.0 Explore P: 0.5685\n",
      "DONE! Episode: 179 Total reward: 64.0 Explore P: 0.5650\n",
      "DONE! Episode: 180 Total reward: 68.0 Explore P: 0.5612\n",
      "DONE! Episode: 181 Total reward: 45.0 Explore P: 0.5587\n",
      "DONE! Episode: 182 Total reward: 87.0 Explore P: 0.5540\n",
      "DONE! Episode: 183 Total reward: 15.0 Explore P: 0.5532\n",
      "DONE! Episode: 184 Total reward: 22.0 Explore P: 0.5520\n",
      "DONE! Episode: 185 Total reward: 107.0 Explore P: 0.5462\n",
      "DONE! Episode: 186 Total reward: 125.0 Explore P: 0.5395\n",
      "DONE! Episode: 187 Total reward: 13.0 Explore P: 0.5389\n",
      "DONE! Episode: 188 Total reward: 194.0 Explore P: 0.5287\n",
      "DONE! Episode: 189 Total reward: 19.0 Explore P: 0.5277\n",
      "DONE! Episode: 190 Total reward: 112.0 Explore P: 0.5219\n",
      "DONE! Episode: 191 Total reward: 48.0 Explore P: 0.5195\n",
      "DONE! Episode: 192 Total reward: 67.0 Explore P: 0.5161\n",
      "DONE! Episode: 193 Total reward: 90.0 Explore P: 0.5116\n",
      "DONE! Episode: 194 Total reward: 112.0 Explore P: 0.5060\n",
      "DONE! Episode: 195 Total reward: 200.0 Explore P: 0.4961\n",
      "Win...\n",
      "DONE! Episode: 196 Total reward: 200.0 Explore P: 0.4865\n",
      "Win...\n",
      "DONE! Episode: 197 Total reward: 200.0 Explore P: 0.4771\n",
      "Win...\n",
      "DONE! Episode: 198 Total reward: 133.0 Explore P: 0.4709\n",
      "DONE! Episode: 199 Total reward: 198.0 Explore P: 0.4619\n",
      "DONE! Episode: 200 Total reward: 92.0 Explore P: 0.4577\n",
      "DONE! Episode: 201 Total reward: 92.0 Explore P: 0.4536\n",
      "DONE! Episode: 202 Total reward: 109.0 Explore P: 0.4488\n",
      "DONE! Episode: 203 Total reward: 200.0 Explore P: 0.4401\n",
      "Win...\n",
      "DONE! Episode: 204 Total reward: 164.0 Explore P: 0.4331\n",
      "DONE! Episode: 205 Total reward: 157.0 Explore P: 0.4265\n",
      "DONE! Episode: 206 Total reward: 100.0 Explore P: 0.4224\n",
      "DONE! Episode: 207 Total reward: 57.0 Explore P: 0.4201\n",
      "DONE! Episode: 208 Total reward: 132.0 Explore P: 0.4147\n",
      "DONE! Episode: 209 Total reward: 157.0 Explore P: 0.4084\n",
      "DONE! Episode: 210 Total reward: 45.0 Explore P: 0.4066\n",
      "DONE! Episode: 211 Total reward: 200.0 Explore P: 0.3987\n",
      "Win...\n",
      "DONE! Episode: 212 Total reward: 58.0 Explore P: 0.3965\n",
      "DONE! Episode: 213 Total reward: 112.0 Explore P: 0.3922\n",
      "DONE! Episode: 214 Total reward: 157.0 Explore P: 0.3862\n",
      "DONE! Episode: 215 Total reward: 191.0 Explore P: 0.3791\n",
      "DONE! Episode: 216 Total reward: 200.0 Explore P: 0.3718\n",
      "Win...\n",
      "DONE! Episode: 217 Total reward: 133.0 Explore P: 0.3670\n",
      "DONE! Episode: 218 Total reward: 170.0 Explore P: 0.3610\n",
      "DONE! Episode: 219 Total reward: 142.0 Explore P: 0.3561\n",
      "DONE! Episode: 220 Total reward: 200.0 Explore P: 0.3492\n",
      "Win...\n",
      "DONE! Episode: 221 Total reward: 200.0 Explore P: 0.3425\n",
      "Win...\n",
      "DONE! Episode: 222 Total reward: 66.0 Explore P: 0.3403\n",
      "DONE! Episode: 223 Total reward: 200.0 Explore P: 0.3338\n",
      "Win...\n",
      "DONE! Episode: 224 Total reward: 183.0 Explore P: 0.3279\n"
     ]
    }
   ],
   "source": [
    "# Initialize the simulation\n",
    "state = env.reset()\n",
    "rewards_per_episode = []\n",
    "\n",
    "step = 0\n",
    "num_consecutive_win = 0\n",
    "\n",
    "# For each episodes\n",
    "for i_episode in range(1, train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % target_updates == 0:\n",
    "        targetQ.set_weights(mainQN.get_weights()) \n",
    "        \n",
    "    # For each iteration per episode.\n",
    "    while t < max_steps:\n",
    "        step += 1\n",
    "     \n",
    "        # Epsilon Greedy Decay\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step)\n",
    "        \n",
    "        # Epsilon Greedy\n",
    "        if explore_p > np.random.rand():\n",
    "            # Make a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            action = mainQN(state)\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "        total_reward += reward\n",
    "\n",
    "        # Episode finish\n",
    "        if done:\n",
    "            # the episode ends so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            t = max_steps\n",
    "\n",
    "            print('DONE! Episode: {}'.format(i_episode),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Explore P: {:.4f}'.format(explore_p))\n",
    "            \n",
    "            rewards_per_episode.append(total_reward)\n",
    "            \n",
    "            # Early Stop\n",
    "            if total_reward >= 199:\n",
    "                num_consecutive_win += 1\n",
    "                print('Win...')\n",
    "            else:\n",
    "                num_consecutive_win = 0                \n",
    "            \n",
    "            if num_consecutive_win > consecutive_win_threshold:                \n",
    "                break\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            env.reset()            \n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "\n",
    "        # Check if we have enough samples on memory to sample\n",
    "        if len(memory) >= batch_size:\n",
    "            # Sample experience from Replay Memory\n",
    "            minibatch = memory.sample(batch_size)\n",
    "        else:\n",
    "            # Keep populating replay memory\n",
    "            print('Populating replay buffer')\n",
    "            continue\n",
    "        \n",
    "        # Get inputs (states) and calculate targets         \n",
    "        inputs = np.zeros((batch_size, 4))\n",
    "        targets = np.zeros((batch_size, 2))\n",
    "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(minibatch):\n",
    "            inputs[i:i+1] = state_b\n",
    "            target = reward_b\n",
    "            # Check if all elements from next next_state_b are different than zero\n",
    "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=0):\n",
    "                target_Q = mainQN.q_values(next_state_b)[0]\n",
    "                target = reward_b + gamma * np.amax(mainQN.q_values(next_state_b)[0])\n",
    "            targets[i] = mainQN.q_values(state_b)\n",
    "            targets[i][action_b] = target\n",
    "        \n",
    "        # Update the model (In each episode end)\n",
    "        mainQN.model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "    \n",
    "    # Stop Earlier ...\n",
    "    if num_consecutive_win > consecutive_win_threshold:\n",
    "        print('Number of consecutive wins high, stop earlier...')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Total Rewards per Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_durations(rewards_per_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "state = env.reset()\n",
    "\n",
    "while True:    \n",
    "    # Act greedly\n",
    "    action = mainQN(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
