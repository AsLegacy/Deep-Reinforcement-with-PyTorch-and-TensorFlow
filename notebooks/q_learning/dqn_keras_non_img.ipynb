{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple DQN on Keras\n",
    "On this example we will do a DQN (Without screen states) on the CartPole problem.\n",
    "\n",
    "### References\n",
    "* https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55\n",
    "* https://stackoverflow.com/questions/51960225/dqn-average-reward-decrease-after-training-for-a-period-of-time\n",
    "* https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 600              # max number of episodes to learn from\n",
    "max_steps = 200                   # max steps in an episode\n",
    "gamma = 0.99                      # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0               # exploration probability at start (Pure exploratory)\n",
    "explore_stop = 0.01               # minimum exploration probability\n",
    "decay_rate = 0.0001               # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 16                  # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.001             # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000               # memory capacity\n",
    "batch_size = 32                   # experience mini-batch size\n",
    "pretrain_length = batch_size*10   # number experiences to pretrain the memory\n",
    "\n",
    "consecutive_win_threshold = 7     # Number of Consecutive wins before stop training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, env, learning_rate=0.01, state_size=4,\n",
    "                 hidden_size=10):\n",
    "        action_size = env.action_space.n\n",
    "        # state inputs to the Q-network\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Dense(hidden_size, activation='relu',\n",
    "                             input_dim=state_size))\n",
    "        self.model.add(Dense(hidden_size, activation='relu'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "\n",
    "        self.optimizer = Adam(lr=learning_rate)#, decay=1e-6)\n",
    "        self.model.compile(loss='mse', optimizer=self.optimizer)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        # Reshape states (,4) --> [1,4]\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        # Get Q-values\n",
    "        Q_values = self.model.predict(state)[0]\n",
    "        # Act greedly\n",
    "        action = np.argmax(Q_values)\n",
    "        return action\n",
    "    \n",
    "    def q_values(self, state):\n",
    "        # Reshape states (,4) --> [1,4]\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        # Get Q-values\n",
    "        Q_values = self.model.predict(state)\n",
    "        return Q_values\n",
    "\n",
    "\n",
    "# Initialize DQN Network\n",
    "mainQN = QNetwork(env, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)),\n",
    "                               size=batch_size,\n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)    \n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "\n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())        \n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 24.0 Explore P: 0.9976\n",
      "Episode: 2 Total reward: 11.0 Explore P: 0.9965\n",
      "Episode: 3 Total reward: 11.0 Explore P: 0.9955\n",
      "Episode: 4 Total reward: 41.0 Explore P: 0.9914\n",
      "Episode: 5 Total reward: 17.0 Explore P: 0.9898\n",
      "Episode: 6 Total reward: 29.0 Explore P: 0.9869\n",
      "Episode: 7 Total reward: 19.0 Explore P: 0.9851\n",
      "Episode: 8 Total reward: 45.0 Explore P: 0.9807\n",
      "Episode: 9 Total reward: 86.0 Explore P: 0.9724\n",
      "Episode: 10 Total reward: 47.0 Explore P: 0.9679\n",
      "Episode: 11 Total reward: 13.0 Explore P: 0.9666\n",
      "Episode: 12 Total reward: 20.0 Explore P: 0.9647\n",
      "Episode: 13 Total reward: 29.0 Explore P: 0.9619\n",
      "Episode: 14 Total reward: 8.0 Explore P: 0.9612\n",
      "Episode: 15 Total reward: 12.0 Explore P: 0.9600\n",
      "Episode: 16 Total reward: 20.0 Explore P: 0.9581\n",
      "Episode: 17 Total reward: 20.0 Explore P: 0.9562\n",
      "Episode: 18 Total reward: 39.0 Explore P: 0.9526\n",
      "Episode: 19 Total reward: 10.0 Explore P: 0.9516\n",
      "Episode: 20 Total reward: 19.0 Explore P: 0.9498\n",
      "Episode: 21 Total reward: 11.0 Explore P: 0.9488\n",
      "Episode: 22 Total reward: 17.0 Explore P: 0.9472\n",
      "Episode: 23 Total reward: 10.0 Explore P: 0.9463\n",
      "Episode: 24 Total reward: 17.0 Explore P: 0.9447\n",
      "Episode: 25 Total reward: 17.0 Explore P: 0.9431\n",
      "Episode: 26 Total reward: 17.0 Explore P: 0.9415\n",
      "Episode: 27 Total reward: 10.0 Explore P: 0.9406\n",
      "Episode: 28 Total reward: 16.0 Explore P: 0.9391\n",
      "Episode: 29 Total reward: 14.0 Explore P: 0.9378\n",
      "Episode: 30 Total reward: 19.0 Explore P: 0.9360\n",
      "Episode: 31 Total reward: 11.0 Explore P: 0.9350\n",
      "Episode: 32 Total reward: 29.0 Explore P: 0.9323\n",
      "Episode: 33 Total reward: 74.0 Explore P: 0.9255\n",
      "Episode: 34 Total reward: 30.0 Explore P: 0.9228\n",
      "Episode: 35 Total reward: 12.0 Explore P: 0.9217\n",
      "Episode: 36 Total reward: 14.0 Explore P: 0.9204\n",
      "Episode: 37 Total reward: 22.0 Explore P: 0.9184\n",
      "Episode: 38 Total reward: 12.0 Explore P: 0.9173\n",
      "Episode: 39 Total reward: 19.0 Explore P: 0.9156\n",
      "Episode: 40 Total reward: 21.0 Explore P: 0.9137\n",
      "Episode: 41 Total reward: 9.0 Explore P: 0.9129\n",
      "Episode: 42 Total reward: 26.0 Explore P: 0.9105\n",
      "Episode: 43 Total reward: 27.0 Explore P: 0.9081\n",
      "Episode: 44 Total reward: 16.0 Explore P: 0.9067\n",
      "Episode: 45 Total reward: 38.0 Explore P: 0.9033\n",
      "Episode: 46 Total reward: 29.0 Explore P: 0.9007\n",
      "Episode: 47 Total reward: 10.0 Explore P: 0.8998\n",
      "Episode: 48 Total reward: 28.0 Explore P: 0.8973\n",
      "Episode: 49 Total reward: 8.0 Explore P: 0.8966\n",
      "Episode: 50 Total reward: 27.0 Explore P: 0.8942\n",
      "Episode: 51 Total reward: 14.0 Explore P: 0.8930\n",
      "Episode: 52 Total reward: 16.0 Explore P: 0.8916\n",
      "Episode: 53 Total reward: 36.0 Explore P: 0.8884\n",
      "Episode: 54 Total reward: 13.0 Explore P: 0.8873\n",
      "Episode: 55 Total reward: 41.0 Explore P: 0.8837\n",
      "Episode: 56 Total reward: 20.0 Explore P: 0.8819\n",
      "Episode: 57 Total reward: 32.0 Explore P: 0.8791\n",
      "Episode: 58 Total reward: 27.0 Explore P: 0.8768\n",
      "Episode: 59 Total reward: 36.0 Explore P: 0.8737\n",
      "Episode: 60 Total reward: 40.0 Explore P: 0.8702\n",
      "Episode: 61 Total reward: 13.0 Explore P: 0.8691\n",
      "Episode: 62 Total reward: 32.0 Explore P: 0.8664\n",
      "Episode: 63 Total reward: 13.0 Explore P: 0.8653\n",
      "Episode: 64 Total reward: 68.0 Explore P: 0.8595\n",
      "Episode: 65 Total reward: 35.0 Explore P: 0.8565\n",
      "Episode: 66 Total reward: 20.0 Explore P: 0.8548\n",
      "Episode: 67 Total reward: 70.0 Explore P: 0.8489\n",
      "Episode: 68 Total reward: 77.0 Explore P: 0.8425\n",
      "Episode: 69 Total reward: 72.0 Explore P: 0.8365\n",
      "Episode: 70 Total reward: 44.0 Explore P: 0.8329\n",
      "Episode: 71 Total reward: 21.0 Explore P: 0.8311\n",
      "Episode: 72 Total reward: 47.0 Explore P: 0.8273\n",
      "Episode: 73 Total reward: 20.0 Explore P: 0.8257\n",
      "Episode: 74 Total reward: 30.0 Explore P: 0.8232\n",
      "Episode: 75 Total reward: 20.0 Explore P: 0.8216\n",
      "Episode: 76 Total reward: 32.0 Explore P: 0.8190\n",
      "Episode: 77 Total reward: 19.0 Explore P: 0.8175\n",
      "Episode: 78 Total reward: 47.0 Explore P: 0.8137\n",
      "Episode: 79 Total reward: 39.0 Explore P: 0.8106\n",
      "Episode: 80 Total reward: 19.0 Explore P: 0.8090\n",
      "Episode: 81 Total reward: 20.0 Explore P: 0.8074\n",
      "Episode: 82 Total reward: 14.0 Explore P: 0.8063\n",
      "Episode: 83 Total reward: 18.0 Explore P: 0.8049\n",
      "Episode: 84 Total reward: 39.0 Explore P: 0.8018\n",
      "Episode: 85 Total reward: 40.0 Explore P: 0.7986\n",
      "Episode: 86 Total reward: 40.0 Explore P: 0.7955\n",
      "Episode: 87 Total reward: 16.0 Explore P: 0.7942\n",
      "Episode: 88 Total reward: 39.0 Explore P: 0.7912\n",
      "Episode: 89 Total reward: 34.0 Explore P: 0.7885\n",
      "Episode: 90 Total reward: 9.0 Explore P: 0.7878\n",
      "Episode: 91 Total reward: 35.0 Explore P: 0.7851\n",
      "Episode: 92 Total reward: 18.0 Explore P: 0.7837\n",
      "Episode: 93 Total reward: 30.0 Explore P: 0.7814\n",
      "Episode: 94 Total reward: 71.0 Explore P: 0.7759\n",
      "Episode: 95 Total reward: 43.0 Explore P: 0.7727\n",
      "Episode: 96 Total reward: 36.0 Explore P: 0.7699\n",
      "Episode: 97 Total reward: 63.0 Explore P: 0.7651\n",
      "Episode: 98 Total reward: 50.0 Explore P: 0.7614\n",
      "Episode: 99 Total reward: 41.0 Explore P: 0.7583\n",
      "Episode: 100 Total reward: 71.0 Explore P: 0.7530\n",
      "Episode: 101 Total reward: 16.0 Explore P: 0.7518\n",
      "Episode: 102 Total reward: 30.0 Explore P: 0.7496\n",
      "Episode: 103 Total reward: 30.0 Explore P: 0.7474\n",
      "Episode: 104 Total reward: 50.0 Explore P: 0.7437\n",
      "Episode: 105 Total reward: 27.0 Explore P: 0.7417\n",
      "Episode: 106 Total reward: 20.0 Explore P: 0.7403\n",
      "Episode: 107 Total reward: 44.0 Explore P: 0.7371\n",
      "Episode: 108 Total reward: 29.0 Explore P: 0.7350\n",
      "Episode: 109 Total reward: 111.0 Explore P: 0.7269\n",
      "Episode: 110 Total reward: 21.0 Explore P: 0.7254\n",
      "Episode: 111 Total reward: 27.0 Explore P: 0.7235\n",
      "Episode: 112 Total reward: 24.0 Explore P: 0.7218\n",
      "Episode: 113 Total reward: 48.0 Explore P: 0.7184\n",
      "Episode: 114 Total reward: 113.0 Explore P: 0.7104\n",
      "Episode: 115 Total reward: 24.0 Explore P: 0.7088\n",
      "Episode: 116 Total reward: 41.0 Explore P: 0.7059\n",
      "Episode: 117 Total reward: 23.0 Explore P: 0.7043\n",
      "Episode: 118 Total reward: 48.0 Explore P: 0.7010\n",
      "Episode: 119 Total reward: 74.0 Explore P: 0.6959\n",
      "Episode: 120 Total reward: 23.0 Explore P: 0.6943\n",
      "Episode: 121 Total reward: 38.0 Explore P: 0.6917\n",
      "Episode: 122 Total reward: 49.0 Explore P: 0.6884\n",
      "Episode: 123 Total reward: 56.0 Explore P: 0.6846\n",
      "Episode: 124 Total reward: 58.0 Explore P: 0.6807\n",
      "Episode: 125 Total reward: 12.0 Explore P: 0.6799\n",
      "Episode: 126 Total reward: 57.0 Explore P: 0.6761\n",
      "Episode: 127 Total reward: 83.0 Explore P: 0.6706\n",
      "Episode: 128 Total reward: 50.0 Explore P: 0.6673\n",
      "Episode: 129 Total reward: 22.0 Explore P: 0.6658\n",
      "Episode: 130 Total reward: 52.0 Explore P: 0.6624\n",
      "Episode: 131 Total reward: 46.0 Explore P: 0.6594\n",
      "Episode: 132 Total reward: 49.0 Explore P: 0.6563\n",
      "Episode: 133 Total reward: 41.0 Explore P: 0.6536\n",
      "Episode: 134 Total reward: 10.0 Explore P: 0.6530\n",
      "Episode: 135 Total reward: 33.0 Explore P: 0.6509\n",
      "Episode: 136 Total reward: 93.0 Explore P: 0.6449\n",
      "Episode: 137 Total reward: 58.0 Explore P: 0.6413\n",
      "Episode: 138 Total reward: 60.0 Explore P: 0.6375\n",
      "Episode: 139 Total reward: 51.0 Explore P: 0.6343\n",
      "Episode: 140 Total reward: 123.0 Explore P: 0.6267\n",
      "Episode: 141 Total reward: 47.0 Explore P: 0.6238\n",
      "Episode: 142 Total reward: 50.0 Explore P: 0.6207\n",
      "Episode: 143 Total reward: 120.0 Explore P: 0.6134\n",
      "Episode: 144 Total reward: 50.0 Explore P: 0.6104\n",
      "Episode: 145 Total reward: 25.0 Explore P: 0.6089\n",
      "Episode: 146 Total reward: 88.0 Explore P: 0.6037\n",
      "Episode: 147 Total reward: 13.0 Explore P: 0.6029\n",
      "Episode: 148 Total reward: 100.0 Explore P: 0.5970\n",
      "Episode: 149 Total reward: 36.0 Explore P: 0.5949\n",
      "Episode: 150 Total reward: 96.0 Explore P: 0.5893\n",
      "Episode: 151 Total reward: 139.0 Explore P: 0.5813\n",
      "Episode: 152 Total reward: 30.0 Explore P: 0.5796\n",
      "Episode: 153 Total reward: 22.0 Explore P: 0.5783\n",
      "Episode: 154 Total reward: 16.0 Explore P: 0.5774\n",
      "Episode: 155 Total reward: 98.0 Explore P: 0.5719\n",
      "Episode: 156 Total reward: 94.0 Explore P: 0.5666\n",
      "Episode: 157 Total reward: 163.0 Explore P: 0.5576\n",
      "Episode: 158 Total reward: 48.0 Explore P: 0.5550\n",
      "Episode: 159 Total reward: 26.0 Explore P: 0.5536\n",
      "Episode: 160 Total reward: 96.0 Explore P: 0.5484\n",
      "Episode: 161 Total reward: 69.0 Explore P: 0.5447\n",
      "Episode: 162 Total reward: 199.0 Explore P: 0.5342\n",
      "Win...\n",
      "Episode: 163 Total reward: 141.0 Explore P: 0.5268\n",
      "Episode: 164 Total reward: 144.0 Explore P: 0.5194\n",
      "Episode: 165 Total reward: 199.0 Explore P: 0.5094\n",
      "Win...\n",
      "Episode: 166 Total reward: 149.0 Explore P: 0.5020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 167 Total reward: 113.0 Explore P: 0.4965\n",
      "Episode: 168 Total reward: 197.0 Explore P: 0.4870\n",
      "Episode: 169 Total reward: 78.0 Explore P: 0.4833\n",
      "Episode: 170 Total reward: 139.0 Explore P: 0.4768\n",
      "Episode: 171 Total reward: 47.0 Explore P: 0.4746\n",
      "Episode: 172 Total reward: 199.0 Explore P: 0.4654\n",
      "Win...\n",
      "Episode: 173 Total reward: 199.0 Explore P: 0.4564\n",
      "Win...\n",
      "Episode: 174 Total reward: 121.0 Explore P: 0.4511\n",
      "Episode: 175 Total reward: 178.0 Explore P: 0.4433\n",
      "Episode: 176 Total reward: 89.0 Explore P: 0.4394\n",
      "Episode: 177 Total reward: 134.0 Explore P: 0.4337\n",
      "Episode: 178 Total reward: 182.0 Explore P: 0.4261\n",
      "Episode: 179 Total reward: 107.0 Explore P: 0.4217\n",
      "Episode: 180 Total reward: 199.0 Explore P: 0.4136\n",
      "Win...\n",
      "Episode: 181 Total reward: 199.0 Explore P: 0.4056\n",
      "Win...\n",
      "Episode: 182 Total reward: 199.0 Explore P: 0.3978\n",
      "Win...\n",
      "Episode: 183 Total reward: 199.0 Explore P: 0.3902\n",
      "Win...\n",
      "Episode: 184 Total reward: 199.0 Explore P: 0.3827\n",
      "Win...\n",
      "Episode: 185 Total reward: 199.0 Explore P: 0.3753\n",
      "Win...\n",
      "Episode: 186 Total reward: 199.0 Explore P: 0.3681\n",
      "Win...\n",
      "Episode: 187 Total reward: 126.0 Explore P: 0.3636\n",
      "Episode: 188 Total reward: 199.0 Explore P: 0.3567\n",
      "Win...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4b402688fdb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Check if all elements from next next_state_b are different than zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_state_b\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mtarget_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ef4f73004a9a>\u001b[0m in \u001b[0;36mq_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Get Q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mQ_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mQ_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1165\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m   \"\"\"\n\u001b[0;32m--> 946\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecuting_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;34m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mscalar_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "num_consecutive_win = 0\n",
    "\n",
    "# For each episodes\n",
    "for ep in range(1, train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    # For each iteration per episode.\n",
    "    while t < max_steps:\n",
    "        step += 1\n",
    "     \n",
    "        # Epsilon Greedy Decay\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step)\n",
    "        \n",
    "        # Epsilon Greedy\n",
    "        if explore_p > np.random.rand():\n",
    "            # Make a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            action = mainQN(state)\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "        total_reward += reward\n",
    "\n",
    "        # Episode finish\n",
    "        if done:\n",
    "            # the episode ends so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            t = max_steps\n",
    "\n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Explore P: {:.4f}'.format(explore_p))\n",
    "            \n",
    "            # Early Stop\n",
    "            if total_reward >= 199:\n",
    "                num_consecutive_win += 1\n",
    "                print('Win...')\n",
    "            else:\n",
    "                num_consecutive_win = 0                \n",
    "            \n",
    "            if num_consecutive_win > consecutive_win_threshold:                \n",
    "                break\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            env.reset()\n",
    "            # Take one random step to get the pole and cart moving\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())            \n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "\n",
    "        # Replay\n",
    "        inputs = np.zeros((batch_size, 4))\n",
    "        targets = np.zeros((batch_size, 2))\n",
    "\n",
    "        # Sample experience from Replay Memory\n",
    "        minibatch = memory.sample(batch_size)\n",
    "        \n",
    "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(minibatch):\n",
    "            inputs[i:i+1] = state_b\n",
    "            target = reward_b\n",
    "            # Check if all elements from next next_state_b are different than zero\n",
    "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=0):\n",
    "                target_Q = mainQN.q_values(next_state_b)[0]\n",
    "                target = reward_b + gamma * np.amax(mainQN.q_values(next_state_b)[0])\n",
    "            targets[i] = mainQN.q_values(state_b)\n",
    "            targets[i][action_b] = target\n",
    "        \n",
    "        # Update the model (In each episode end)\n",
    "        mainQN.model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "    \n",
    "    # Stop Earlier ...\n",
    "    if num_consecutive_win > consecutive_win_threshold:\n",
    "        print('Number of consecutive wins high, stop earlier...')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "state = env.reset()\n",
    "\n",
    "while True:    \n",
    "    # Act greedly\n",
    "    action = mainQN(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
