{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple DQN on Keras\n",
    "The DQN Algorithm solves the issues related to the function approximations with Neural Networks:\n",
    "* Bootstrap issue (Estimation with an estimation)\n",
    "* Uncorrelate samples of experiences to facilitate Gradient Descent\n",
    "\n",
    "![alt text](imgs/DQN.png \"DQN\")\n",
    "\n",
    "<img src=\"imgs/dqn_algo.png\" alt=\"Algorithm DQN\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Act Greedy\n",
    "$\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$\n",
    "\n",
    "### Target Error\n",
    "$\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))$\n",
    "\n",
    "### References\n",
    "* https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55\n",
    "* https://stackoverflow.com/questions/51960225/dqn-average-reward-decrease-after-training-for-a-period-of-time\n",
    "* https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682\n",
    "* https://towardsdatascience.com/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f\n",
    "* https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardo_a/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Debug\n",
    "import ipdb; \n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "## Plot function\n",
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def plot_durations(durations_t):\n",
    "    plt.figure(2)\n",
    "    plt.clf()    \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Rewards')\n",
    "    plt.plot(durations_t)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:        \n",
    "        plt.plot(moving_average(durations_t,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 600              # max number of episodes to learn from\n",
    "max_steps = 200                   # max steps in an episode\n",
    "gamma = 0.99                      # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0               # exploration probability at start (Pure exploratory)\n",
    "explore_stop = 0.01               # minimum exploration probability\n",
    "decay_rate = 0.0001               # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 16                  # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.001             # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000               # memory capacity\n",
    "batch_size = 32                   # experience mini-batch size\n",
    "pretrain_length = batch_size*10   # number experiences to pretrain the memory\n",
    "\n",
    "consecutive_win_threshold = 6     # Number of Consecutive wins before stop training\n",
    "target_updates = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, env, learning_rate=0.01, state_size=4,\n",
    "                 hidden_size=10):\n",
    "        action_size = env.action_space.n\n",
    "        # state inputs to the Q-network\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Dense(hidden_size, activation='relu',\n",
    "                             input_dim=state_size))\n",
    "        self.model.add(Dense(hidden_size, activation='relu'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "\n",
    "        self.optimizer = Adam(lr=learning_rate)#, decay=1e-6)\n",
    "        self.model.compile(loss='mse', optimizer=self.optimizer)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        # Reshape states (,4) --> [1,4]\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        # Get Q-values\n",
    "        Q_values = self.model.predict(state)[0]\n",
    "        # Act greedly\n",
    "        action = np.argmax(Q_values)\n",
    "        return action\n",
    "    \n",
    "    def q_values(self, state):\n",
    "        # Reshape states (,4) --> [1,4]\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        # Get Q-values\n",
    "        Q_values = self.model.predict(state)\n",
    "        return Q_values\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.model.set_weights(weights)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "\n",
    "# Initialize DQN Network\n",
    "mainQN = QNetwork(env, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "targetQ = QNetwork(env, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "targetQ.set_weights(mainQN.get_weights()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)),\n",
    "                               size=batch_size,\n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "DONE! Episode: 1 Total reward: 12.0 Explore P: 0.9988\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "DONE! Episode: 2 Total reward: 13.0 Explore P: 0.9975\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "Populating replay buffer\n",
      "> \u001b[0;32m<ipython-input-5-d0aa7ed9d5b6>\u001b[0m(85)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     84 \u001b[0;31m            \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 85 \u001b[0;31m            \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     86 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> state_b.shape\n",
      "(4,)\n",
      "ipdb> state_b\n",
      "array([ 0.02168021, -0.21501691,  0.00332558,  0.27808935])\n",
      "ipdb> inputs.shape\n",
      "(32, 4)\n",
      "ipdb> inputs[i:i+1]\n",
      "array([[0., 0., 0., 0.]])\n",
      "ipdb> inputs[i]\n",
      "array([0., 0., 0., 0.])\n",
      "ipdb> i\n",
      "0\n",
      "ipdb> inputs[i:i+2]\n",
      "array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]])\n",
      "ipdb> inputs[i] = state_b\n",
      "ipdb> inputs[i]\n",
      "array([ 0.02168021, -0.21501691,  0.00332558,  0.27808935])\n",
      "ipdb> inputs[i:i+1] = state_b\n",
      "ipdb> inputs[i:i+1]\n",
      "array([[ 0.02168021, -0.21501691,  0.00332558,  0.27808935]])\n",
      "ipdb> inputs[i:i+1].shape\n",
      "(1, 4)\n",
      "ipdb> state_b.shape\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the simulation\n",
    "state = env.reset()\n",
    "rewards_per_episode = []\n",
    "\n",
    "step = 0\n",
    "num_consecutive_win = 0\n",
    "\n",
    "# For each episodes\n",
    "for i_episode in range(1, train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % target_updates == 0:\n",
    "        targetQ.set_weights(mainQN.get_weights()) \n",
    "        \n",
    "    # For each iteration per episode.\n",
    "    while t < max_steps:\n",
    "        step += 1\n",
    "     \n",
    "        # Epsilon Greedy Decay\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step)\n",
    "        \n",
    "        # Epsilon Greedy\n",
    "        if explore_p > np.random.rand():\n",
    "            # Make a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            action = mainQN(state)\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "        total_reward += reward\n",
    "\n",
    "        # Episode finish\n",
    "        if done:\n",
    "            # the episode ends so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            t = max_steps\n",
    "\n",
    "            print('DONE! Episode: {}'.format(i_episode),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Explore P: {:.4f}'.format(explore_p))\n",
    "            \n",
    "            rewards_per_episode.append(total_reward)\n",
    "            \n",
    "            # Early Stop\n",
    "            if total_reward >= 199:\n",
    "                num_consecutive_win += 1\n",
    "                print('Win...')\n",
    "            else:\n",
    "                num_consecutive_win = 0                \n",
    "            \n",
    "            if num_consecutive_win > consecutive_win_threshold:                \n",
    "                break\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            env.reset()            \n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "\n",
    "        # Check if we have enough samples on memory to sample\n",
    "        if len(memory) >= batch_size:\n",
    "            # Sample experience from Replay Memory\n",
    "            minibatch = memory.sample(batch_size)\n",
    "        else:\n",
    "            # Keep populating replay memory\n",
    "            print('Populating replay buffer')\n",
    "            continue\n",
    "        \n",
    "        # Instantiate inputs (states) and targets         \n",
    "        inputs = np.zeros((batch_size, np.prod(env.observation_space.shape)))\n",
    "        targets = np.zeros((batch_size, env.action_space.n))\n",
    "        \n",
    "        \n",
    "        # Iterate on the minibatch of samples from the replay memory\n",
    "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(minibatch):\n",
    "            #ipdb.set_trace()\n",
    "            inputs[i:i+1] = state_b\n",
    "            \n",
    "            # Mark target as immediate reward (In case the episode ended)\n",
    "            target = reward_b\n",
    "            \n",
    "            # Calculate target if episoded ended.\n",
    "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=0):    \n",
    "                #ipdb.set_trace()\n",
    "                target = reward_b + gamma * np.max(targetQ.q_values(next_state_b))\n",
    "            targets[i] = targetQ.q_values(state_b)\n",
    "            targets[i][action_b] = target\n",
    "        \n",
    "        # Update the model (In each episode end)\n",
    "        mainQN.model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "    \n",
    "    # Stop Earlier ...\n",
    "    if num_consecutive_win > consecutive_win_threshold:\n",
    "        print('Number of consecutive wins high, stop earlier...')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Total Rewards per Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_durations(rewards_per_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "state = env.reset()\n",
    "\n",
    "while True:    \n",
    "    # Act greedly\n",
    "    action = mainQN(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
