{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple DQN on Keras\n",
    "On this example we will do a DQN (Without screen states) on the CartPole problem.\n",
    "\n",
    "### References\n",
    "* https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55\n",
    "* https://stackoverflow.com/questions/51960225/dqn-average-reward-decrease-after-training-for-a-period-of-time\n",
    "* https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 600              # max number of episodes to learn from\n",
    "max_steps = 200                   # max steps in an episode\n",
    "gamma = 0.99                      # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0               # exploration probability at start (Pure exploratory)\n",
    "explore_stop = 0.01               # minimum exploration probability\n",
    "decay_rate = 0.0001               # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 16                  # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.001             # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000               # memory capacity\n",
    "batch_size = 32                   # experience mini-batch size\n",
    "pretrain_length = batch_size*10   # number experiences to pretrain the memory\n",
    "\n",
    "consecutive_win_threshold = 8     # Number of Consecutive wins before stop training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, env, learning_rate=0.01, state_size=4,\n",
    "                 hidden_size=10):\n",
    "        action_size = env.action_space.n\n",
    "        # state inputs to the Q-network\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Dense(hidden_size, activation='relu',\n",
    "                             input_dim=state_size))\n",
    "        self.model.add(Dense(hidden_size, activation='relu'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "\n",
    "        self.optimizer = Adam(lr=learning_rate)#, decay=1e-6)\n",
    "        self.model.compile(loss='mse', optimizer=self.optimizer)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        # Reshape states (,4) --> [1,4]\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        # Get Q-values\n",
    "        Q_values = self.model.predict(state)[0]\n",
    "        # Act greedly\n",
    "        action = np.argmax(Q_values)\n",
    "        return action\n",
    "\n",
    "\n",
    "# Initialize DQN Network\n",
    "mainQN = QNetwork(env, hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)),\n",
    "                               size=batch_size,\n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "\n",
    "memory = Memory(max_size=memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "state = np.reshape(state, [1, 4])\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, 4])\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "\n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "        state = np.reshape(state, [1, 4])\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 9.0 Explore P: 0.9991\n",
      "Episode: 2 Total reward: 13.0 Explore P: 0.9978\n",
      "Episode: 3 Total reward: 69.0 Explore P: 0.9910\n",
      "Episode: 4 Total reward: 17.0 Explore P: 0.9894\n",
      "Episode: 5 Total reward: 41.0 Explore P: 0.9854\n",
      "Episode: 6 Total reward: 24.0 Explore P: 0.9830\n",
      "Episode: 7 Total reward: 15.0 Explore P: 0.9816\n",
      "Episode: 8 Total reward: 18.0 Explore P: 0.9798\n",
      "Episode: 9 Total reward: 21.0 Explore P: 0.9778\n",
      "Episode: 10 Total reward: 33.0 Explore P: 0.9746\n",
      "Episode: 11 Total reward: 34.0 Explore P: 0.9713\n",
      "Episode: 12 Total reward: 18.0 Explore P: 0.9696\n",
      "Episode: 13 Total reward: 11.0 Explore P: 0.9685\n",
      "Episode: 14 Total reward: 33.0 Explore P: 0.9654\n",
      "Episode: 15 Total reward: 17.0 Explore P: 0.9638\n",
      "Episode: 16 Total reward: 30.0 Explore P: 0.9609\n",
      "Episode: 17 Total reward: 10.0 Explore P: 0.9599\n",
      "Episode: 18 Total reward: 20.0 Explore P: 0.9580\n",
      "Episode: 19 Total reward: 51.0 Explore P: 0.9532\n",
      "Episode: 20 Total reward: 15.0 Explore P: 0.9518\n",
      "Episode: 21 Total reward: 37.0 Explore P: 0.9483\n",
      "Episode: 22 Total reward: 14.0 Explore P: 0.9470\n",
      "Episode: 23 Total reward: 37.0 Explore P: 0.9436\n",
      "Episode: 24 Total reward: 23.0 Explore P: 0.9414\n",
      "Episode: 25 Total reward: 9.0 Explore P: 0.9406\n",
      "Episode: 26 Total reward: 16.0 Explore P: 0.9391\n",
      "Episode: 27 Total reward: 28.0 Explore P: 0.9365\n",
      "Episode: 28 Total reward: 39.0 Explore P: 0.9329\n",
      "Episode: 29 Total reward: 13.0 Explore P: 0.9317\n",
      "Episode: 30 Total reward: 18.0 Explore P: 0.9300\n",
      "Episode: 31 Total reward: 12.0 Explore P: 0.9289\n",
      "Episode: 32 Total reward: 19.0 Explore P: 0.9272\n",
      "Episode: 33 Total reward: 36.0 Explore P: 0.9239\n",
      "Episode: 34 Total reward: 24.0 Explore P: 0.9217\n",
      "Episode: 35 Total reward: 27.0 Explore P: 0.9192\n",
      "Episode: 36 Total reward: 19.0 Explore P: 0.9175\n",
      "Episode: 37 Total reward: 34.0 Explore P: 0.9144\n",
      "Episode: 38 Total reward: 15.0 Explore P: 0.9131\n",
      "Episode: 39 Total reward: 22.0 Explore P: 0.9111\n",
      "Episode: 40 Total reward: 13.0 Explore P: 0.9099\n",
      "Episode: 41 Total reward: 18.0 Explore P: 0.9083\n",
      "Episode: 42 Total reward: 19.0 Explore P: 0.9066\n",
      "Episode: 43 Total reward: 48.0 Explore P: 0.9023\n",
      "Episode: 44 Total reward: 11.0 Explore P: 0.9013\n",
      "Episode: 45 Total reward: 22.0 Explore P: 0.8994\n",
      "Episode: 46 Total reward: 15.0 Explore P: 0.8980\n",
      "Episode: 47 Total reward: 18.0 Explore P: 0.8964\n",
      "Episode: 48 Total reward: 28.0 Explore P: 0.8940\n",
      "Episode: 49 Total reward: 35.0 Explore P: 0.8909\n",
      "Episode: 50 Total reward: 18.0 Explore P: 0.8893\n",
      "Episode: 51 Total reward: 11.0 Explore P: 0.8883\n",
      "Episode: 52 Total reward: 13.0 Explore P: 0.8872\n",
      "Episode: 53 Total reward: 18.0 Explore P: 0.8856\n",
      "Episode: 54 Total reward: 14.0 Explore P: 0.8844\n",
      "Episode: 55 Total reward: 42.0 Explore P: 0.8807\n",
      "Episode: 56 Total reward: 16.0 Explore P: 0.8793\n",
      "Episode: 57 Total reward: 14.0 Explore P: 0.8781\n",
      "Episode: 58 Total reward: 15.0 Explore P: 0.8768\n",
      "Episode: 59 Total reward: 19.0 Explore P: 0.8752\n",
      "Episode: 60 Total reward: 73.0 Explore P: 0.8689\n",
      "Episode: 61 Total reward: 18.0 Explore P: 0.8673\n",
      "Episode: 62 Total reward: 20.0 Explore P: 0.8656\n",
      "Episode: 63 Total reward: 13.0 Explore P: 0.8645\n",
      "Episode: 64 Total reward: 11.0 Explore P: 0.8636\n",
      "Episode: 65 Total reward: 10.0 Explore P: 0.8627\n",
      "Episode: 66 Total reward: 27.0 Explore P: 0.8604\n",
      "Episode: 67 Total reward: 44.0 Explore P: 0.8567\n",
      "Episode: 68 Total reward: 20.0 Explore P: 0.8550\n",
      "Episode: 69 Total reward: 48.0 Explore P: 0.8509\n",
      "Episode: 70 Total reward: 19.0 Explore P: 0.8493\n",
      "Episode: 71 Total reward: 13.0 Explore P: 0.8482\n",
      "Episode: 72 Total reward: 55.0 Explore P: 0.8436\n",
      "Episode: 73 Total reward: 25.0 Explore P: 0.8416\n",
      "Episode: 74 Total reward: 34.0 Explore P: 0.8387\n",
      "Episode: 75 Total reward: 10.0 Explore P: 0.8379\n",
      "Episode: 76 Total reward: 20.0 Explore P: 0.8363\n",
      "Episode: 77 Total reward: 19.0 Explore P: 0.8347\n",
      "Episode: 78 Total reward: 54.0 Explore P: 0.8302\n",
      "Episode: 79 Total reward: 15.0 Explore P: 0.8290\n",
      "Episode: 80 Total reward: 22.0 Explore P: 0.8272\n",
      "Episode: 81 Total reward: 15.0 Explore P: 0.8260\n",
      "Episode: 82 Total reward: 32.0 Explore P: 0.8234\n",
      "Episode: 83 Total reward: 20.0 Explore P: 0.8218\n",
      "Episode: 84 Total reward: 20.0 Explore P: 0.8201\n",
      "Episode: 85 Total reward: 19.0 Explore P: 0.8186\n",
      "Episode: 86 Total reward: 46.0 Explore P: 0.8149\n",
      "Episode: 87 Total reward: 18.0 Explore P: 0.8134\n",
      "Episode: 88 Total reward: 33.0 Explore P: 0.8108\n",
      "Episode: 89 Total reward: 18.0 Explore P: 0.8094\n",
      "Episode: 90 Total reward: 77.0 Explore P: 0.8032\n",
      "Episode: 91 Total reward: 50.0 Explore P: 0.7993\n",
      "Episode: 92 Total reward: 11.0 Explore P: 0.7984\n",
      "Episode: 93 Total reward: 26.0 Explore P: 0.7964\n",
      "Episode: 94 Total reward: 44.0 Explore P: 0.7929\n",
      "Episode: 95 Total reward: 65.0 Explore P: 0.7878\n",
      "Episode: 96 Total reward: 81.0 Explore P: 0.7816\n",
      "Episode: 97 Total reward: 13.0 Explore P: 0.7806\n",
      "Episode: 98 Total reward: 21.0 Explore P: 0.7789\n",
      "Episode: 99 Total reward: 115.0 Explore P: 0.7701\n",
      "Episode: 100 Total reward: 40.0 Explore P: 0.7671\n",
      "Episode: 101 Total reward: 33.0 Explore P: 0.7646\n",
      "Episode: 102 Total reward: 56.0 Explore P: 0.7604\n",
      "Episode: 103 Total reward: 70.0 Explore P: 0.7552\n",
      "Episode: 104 Total reward: 57.0 Explore P: 0.7509\n",
      "Episode: 105 Total reward: 80.0 Explore P: 0.7450\n",
      "Episode: 106 Total reward: 43.0 Explore P: 0.7419\n",
      "Episode: 107 Total reward: 13.0 Explore P: 0.7409\n",
      "Episode: 108 Total reward: 56.0 Explore P: 0.7368\n",
      "Episode: 109 Total reward: 197.0 Explore P: 0.7227\n",
      "Episode: 110 Total reward: 57.0 Explore P: 0.7186\n",
      "Episode: 111 Total reward: 163.0 Explore P: 0.7072\n",
      "Episode: 112 Total reward: 91.0 Explore P: 0.7008\n",
      "Episode: 113 Total reward: 47.0 Explore P: 0.6976\n",
      "Episode: 114 Total reward: 119.0 Explore P: 0.6895\n",
      "Episode: 115 Total reward: 62.0 Explore P: 0.6853\n",
      "Episode: 116 Total reward: 53.0 Explore P: 0.6817\n",
      "Episode: 117 Total reward: 50.0 Explore P: 0.6783\n",
      "Episode: 118 Total reward: 76.0 Explore P: 0.6733\n",
      "Episode: 119 Total reward: 25.0 Explore P: 0.6716\n",
      "Episode: 120 Total reward: 18.0 Explore P: 0.6704\n",
      "Episode: 121 Total reward: 117.0 Explore P: 0.6628\n",
      "Episode: 122 Total reward: 48.0 Explore P: 0.6596\n",
      "Episode: 123 Total reward: 29.0 Explore P: 0.6578\n",
      "Episode: 124 Total reward: 16.0 Explore P: 0.6567\n",
      "Episode: 125 Total reward: 36.0 Explore P: 0.6544\n",
      "Episode: 126 Total reward: 59.0 Explore P: 0.6506\n",
      "Episode: 127 Total reward: 75.0 Explore P: 0.6458\n",
      "Episode: 128 Total reward: 62.0 Explore P: 0.6419\n",
      "Episode: 129 Total reward: 157.0 Explore P: 0.6320\n",
      "Episode: 130 Total reward: 48.0 Explore P: 0.6291\n",
      "Episode: 131 Total reward: 73.0 Explore P: 0.6246\n",
      "Episode: 132 Total reward: 178.0 Explore P: 0.6137\n",
      "Episode: 133 Total reward: 15.0 Explore P: 0.6128\n",
      "Episode: 134 Total reward: 32.0 Explore P: 0.6109\n",
      "Episode: 135 Total reward: 74.0 Explore P: 0.6065\n",
      "Episode: 136 Total reward: 16.0 Explore P: 0.6055\n",
      "Episode: 137 Total reward: 64.0 Explore P: 0.6017\n",
      "Episode: 138 Total reward: 52.0 Explore P: 0.5986\n",
      "Episode: 139 Total reward: 199.0 Explore P: 0.5870\n",
      "Consecutive Win...\n",
      "Episode: 140 Total reward: 191.0 Explore P: 0.5761\n",
      "Episode: 141 Total reward: 67.0 Explore P: 0.5723\n",
      "Episode: 142 Total reward: 91.0 Explore P: 0.5672\n",
      "Episode: 143 Total reward: 199.0 Explore P: 0.5563\n",
      "Consecutive Win...\n",
      "Episode: 144 Total reward: 57.0 Explore P: 0.5532\n",
      "Episode: 145 Total reward: 199.0 Explore P: 0.5425\n",
      "Consecutive Win...\n",
      "Episode: 146 Total reward: 37.0 Explore P: 0.5405\n",
      "Episode: 147 Total reward: 16.0 Explore P: 0.5396\n",
      "Episode: 148 Total reward: 199.0 Explore P: 0.5292\n",
      "Consecutive Win...\n",
      "Episode: 149 Total reward: 199.0 Explore P: 0.5190\n",
      "Consecutive Win...\n",
      "Episode: 150 Total reward: 66.0 Explore P: 0.5156\n",
      "Episode: 151 Total reward: 186.0 Explore P: 0.5063\n",
      "Episode: 152 Total reward: 143.0 Explore P: 0.4993\n",
      "Episode: 153 Total reward: 17.0 Explore P: 0.4984\n",
      "Episode: 154 Total reward: 27.0 Explore P: 0.4971\n",
      "Episode: 155 Total reward: 199.0 Explore P: 0.4875\n",
      "Consecutive Win...\n",
      "Episode: 156 Total reward: 199.0 Explore P: 0.4781\n",
      "Consecutive Win...\n",
      "Episode: 157 Total reward: 199.0 Explore P: 0.4689\n",
      "Consecutive Win...\n",
      "Episode: 158 Total reward: 44.0 Explore P: 0.4669\n",
      "Episode: 159 Total reward: 121.0 Explore P: 0.4614\n",
      "Episode: 160 Total reward: 199.0 Explore P: 0.4525\n",
      "Consecutive Win...\n",
      "Episode: 161 Total reward: 34.0 Explore P: 0.4510\n",
      "Episode: 162 Total reward: 191.0 Explore P: 0.4426\n",
      "Episode: 163 Total reward: 199.0 Explore P: 0.4341\n",
      "Consecutive Win...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 164 Total reward: 199.0 Explore P: 0.4258\n",
      "Consecutive Win...\n",
      "Episode: 165 Total reward: 199.0 Explore P: 0.4176\n",
      "Consecutive Win...\n",
      "Episode: 166 Total reward: 199.0 Explore P: 0.4095\n",
      "Consecutive Win...\n",
      "Episode: 167 Total reward: 136.0 Explore P: 0.4041\n",
      "Episode: 168 Total reward: 199.0 Explore P: 0.3964\n",
      "Consecutive Win...\n",
      "Episode: 169 Total reward: 199.0 Explore P: 0.3888\n",
      "Consecutive Win...\n",
      "Episode: 170 Total reward: 199.0 Explore P: 0.3813\n",
      "Consecutive Win...\n",
      "Episode: 171 Total reward: 199.0 Explore P: 0.3740\n",
      "Consecutive Win...\n",
      "Episode: 172 Total reward: 197.0 Explore P: 0.3669\n",
      "Episode: 173 Total reward: 199.0 Explore P: 0.3599\n",
      "Consecutive Win...\n",
      "Episode: 174 Total reward: 199.0 Explore P: 0.3530\n",
      "Consecutive Win...\n",
      "Episode: 175 Total reward: 172.0 Explore P: 0.3471\n",
      "Episode: 176 Total reward: 199.0 Explore P: 0.3405\n",
      "Consecutive Win...\n",
      "Episode: 177 Total reward: 199.0 Explore P: 0.3340\n",
      "Consecutive Win...\n",
      "Episode: 178 Total reward: 199.0 Explore P: 0.3276\n",
      "Consecutive Win...\n",
      "Episode: 179 Total reward: 199.0 Explore P: 0.3213\n",
      "Consecutive Win...\n",
      "Episode: 180 Total reward: 199.0 Explore P: 0.3152\n",
      "Consecutive Win...\n",
      "Episode: 181 Total reward: 199.0 Explore P: 0.3092\n",
      "Consecutive Win...\n",
      "Episode: 182 Total reward: 199.0 Explore P: 0.3033\n",
      "Consecutive Win...\n",
      "Episode: 183 Total reward: 199.0 Explore P: 0.2975\n",
      "Consecutive Win...\n",
      "Episode: 184 Total reward: 199.0 Explore P: 0.2918\n",
      "Consecutive Win...\n",
      "Number of consecutive wins high, stop earlier...\n",
      "Number of consecutive wins high, stop earlier...\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "num_consecutive_win = 0\n",
    "\n",
    "# For each episodes\n",
    "for ep in range(1, train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    # For each iteration per episode.\n",
    "    while t < max_steps:\n",
    "        step += 1\n",
    "     \n",
    "        # Epsilon Greedy Decay\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step)\n",
    "        \n",
    "        # Epsilon Greedy\n",
    "        if explore_p > np.random.rand():\n",
    "            # Make a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            Qs = mainQN.model.predict(state)[0]\n",
    "            action = np.argmax(Qs)\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        total_reward += reward\n",
    "\n",
    "        # Episode finish\n",
    "        if done:\n",
    "            # the episode ends so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            t = max_steps\n",
    "\n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Explore P: {:.4f}'.format(explore_p))\n",
    "            \n",
    "            # Early Stop\n",
    "            if total_reward >= 199:\n",
    "                num_consecutive_win += 1\n",
    "                print('Consecutive Win...')\n",
    "            else:\n",
    "                num_consecutive_win = 0                \n",
    "            \n",
    "            if num_consecutive_win > consecutive_win_threshold:                \n",
    "                break\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            env.reset()\n",
    "            # Take one random step to get the pole and cart moving\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            state = np.reshape(state, [1, 4])\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "\n",
    "        # Replay\n",
    "        inputs = np.zeros((batch_size, 4))\n",
    "        targets = np.zeros((batch_size, 2))\n",
    "\n",
    "        # Sample experience from Replay Memory\n",
    "        minibatch = memory.sample(batch_size)\n",
    "        \n",
    "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(minibatch):\n",
    "            inputs[i:i+1] = state_b\n",
    "            target = reward_b\n",
    "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):\n",
    "                target_Q = mainQN.model.predict(next_state_b)[0]\n",
    "                target = reward_b + gamma * np.amax(mainQN.model.predict(next_state_b)[0])\n",
    "            targets[i] = mainQN.model.predict(state_b)\n",
    "            targets[i][action_b] = target\n",
    "        \n",
    "        # Update the model (In each episode end)\n",
    "        mainQN.model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "    \n",
    "    # Stop Earlier ...\n",
    "    if num_consecutive_win > consecutive_win_threshold:\n",
    "        print('Number of consecutive wins high, stop earlier...')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Learned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "state = env.reset()\n",
    "\n",
    "while True:    \n",
    "    # Act greedly\n",
    "    action = mainQN(state)\n",
    "\n",
    "    # Uncomment to render the visual state in a window\n",
    "    env.render()\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
