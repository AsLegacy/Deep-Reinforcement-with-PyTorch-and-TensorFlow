{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning\n",
    "## Q Learning refresher\n",
    "The simplest version of Q-Learning (Tabular Q-learning) holds a table of (state x actions) that is populated by the agent having experiences with the environment. The idea is to learn the Action Value Function $Q(s,a)$ through experiences and their rewards.\n",
    "\n",
    "The only problem of this approach is that holding a table for enviroments that have lots of states will be intractable, to solve this issue we use a function approximator (Neural Network) that will generalize and learn a parametrized action value function $Q_{\\theta}(s,a)$.\n",
    "\n",
    "### Loss Function\n",
    "We can use as loss function the Mean Squared error between the model prediction of the Q value and a target calculated with the Bellman Equation\n",
    "![alt text](imgs/loss_func_aprox.png \"Game\")\n",
    "\n",
    "### Q Learning Update equation\n",
    "The update equation will provides us the target for our loss function.\n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big[r_t + \\gamma \\max\\limits_{a} Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\big]$$\n",
    "```python\n",
    "Q_target[action] = reward + torch.mul(maxQ1.detach(), gamma)\n",
    "```\n",
    "\n",
    "\n",
    "#### References\n",
    "* https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4\n",
    "* https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8\n",
    "* https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "* https://github.com/blackredscarf/pytorch-DQN\n",
    "* https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55\n",
    "* https://www.toptal.com/deep-learning/pytorch-reinforcement-learning-tutorial\n",
    "* https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    "* https://keon.io/deep-q-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import glob, os, time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyper parameters\n",
    "epsilon = 0.3\n",
    "gamma = 0.99\n",
    "learning_rate = 0.001\n",
    "max_position = -0.4\n",
    "loss_history = []\n",
    "reward_history = []\n",
    "episodes = 2000\n",
    "successes = 0\n",
    "position = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Mountain Car v0 Environment\n",
    "The problem we will be dealing with is the Mountain Car environment, where you need to bring the car to the position 0.5.\n",
    "\n",
    "#### States\n",
    "Num | Observation  | Min  | Max  \n",
    "----|--------------|------|----   \n",
    "0   | position     | -1.2 | 0.6\n",
    "1   | velocity     | -0.07| 0.07\n",
    "\n",
    "#### Actions\n",
    "Num | Action|\n",
    "----|-------------|\n",
    "0   | push left   |\n",
    "1   | no push     |\n",
    "2   | push right  |\n",
    "\n",
    "#### Reward\n",
    "-1 for each time step, until the goal position of 0.5 is reached. As with MountainCarContinuous v0, there is no penalty for climbing the left hill, which upon reached acts as a wall.\n",
    "\n",
    "#### Episode Termination\n",
    "The episode ends when you reach 0.5 position, or if 200 iterations are reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "# Set fixed random seeds\n",
    "env.seed(3); np.random.seed(3); tf.random.set_seed(1)\n",
    "# Writer for tensorboard\n",
    "writer = SummaryWriter('./tboardlogs/{}'.format(datetime.now().strftime('%b%d_%H-%M-%S')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Model for Q Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_func_approx:\n",
    "    def __init__(self):\n",
    "        #super(Q_func_approx, self).__init__()\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.hidden = 100\n",
    "        \n",
    "        \n",
    "        #self.layer1 = nn.Linear(self.state_space, self.hidden, bias=False)\n",
    "        #self.layer2 = nn.Linear(self.hidden, self.action_space, bias=False)\n",
    "        \n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.hidden, use_bias=False),\n",
    "            tf.keras.layers.Dense(self.action_space, use_bias=False)\n",
    "        ])\n",
    "    \n",
    "    #@tf.function\n",
    "    def forward(self, x, is_training=True):   \n",
    "        # Convert state to tensor\n",
    "        #state = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "        #res = self.layer1(state)\n",
    "        #res = self.layer2(res)\n",
    "        x = x[np.newaxis, :]\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        res = self.model(x, training=is_training)\n",
    "        return res[0]\n",
    "    \n",
    "    \n",
    "    #@tf.function\n",
    "    def train(self, loss_fn, optimizer, Q, Q_target):\n",
    "        loss = loss_fn(Q, Q_target)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q Function Approximation\n",
    "q_func = Q_func_approx()\n",
    "\n",
    "# Initialize Loss function as Mean Squared Error\n",
    "#loss_fn = nn.MSELoss()\n",
    "loss_fn = tf.losses.MSE\n",
    "\n",
    "# Initialize Optimizer and Scheduler\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "scheduler = tf.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "    decay_steps=1,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "#optimizer = optim.SGD(q_func.parameters(), lr=learning_rate)\n",
    "optimizer = tf.optimizers.SGD(learning_rate=scheduler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Q Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for episode in trange(episodes):\n",
    "    episode_loss = 0\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # Get first action value function\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            Q = q_func.forward(state)\n",
    "            \n",
    "            # Choose epsilon-greedy action\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                # Sample something from the action space\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Just act greedly\n",
    "                action = tf.math.argmax(Q)\n",
    "                action = action.numpy()\n",
    "\n",
    "            # Step forward and receive next state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "            # Find max Q for t+1 state\n",
    "            Q1 = q_func.forward(next_state)\n",
    "            maxQ1 = tf.math.maximum(*Q1)\n",
    "\n",
    "            # Create target Q value for training the Q_function approximation model\n",
    "            Q_target = Q.__copy__().numpy() \n",
    "            Q_target[action] = reward + maxQ1*gamma\n",
    "\n",
    "            # Calculate loss\n",
    "            #q_func.zero_grad()\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "\n",
    "            loss = q_func.train(loss_fn, optimizer, Q, Q_target)\n",
    "\n",
    "\n",
    "            # Record history\n",
    "            episode_loss += loss.numpy()\n",
    "            episode_reward += reward\n",
    "            # Keep track of max position\n",
    "            if next_state[0] > max_position:\n",
    "                max_position = next_state[0]\n",
    "                writer.add_scalar('data/max_position', max_position, episode)\n",
    "\n",
    "            if done:\n",
    "                if next_state[0] >= 0.5:\n",
    "                    # On successful epsisodes, adjust the following parameters\n",
    "\n",
    "                    # Adjust epsilon\n",
    "                    epsilon *= .95\n",
    "                    writer.add_scalar('data/epsilon', epsilon, episode)\n",
    "\n",
    "                    # Adjust learning rate\n",
    "                    scheduler.step()\n",
    "                    writer.add_scalar('data/learning_rate', optimizer.param_groups[0]['lr'], episode)\n",
    "\n",
    "                    # Record successful episode\n",
    "                    successes += 1\n",
    "                    writer.add_scalar('data/cumulative_success', successes, episode)\n",
    "                    writer.add_scalar('data/success', 1, episode)\n",
    "\n",
    "                elif next_state[0] < 0.5:\n",
    "                    writer.add_scalar('data/success', 0, episode)\n",
    "\n",
    "                # Record history on tensorboard\n",
    "                loss_history.append(episode_loss)\n",
    "                reward_history.append(episode_reward)\n",
    "                writer.add_scalar('data/episode_loss', episode_loss, episode)\n",
    "                writer.add_scalar('data/episode_reward', episode_reward, episode)\n",
    "                writer.add_scalar('data/position', next_state[0], episode)\n",
    "                position.append(next_state[0])\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                state = next_state\n",
    "            \n",
    "writer.close()\n",
    "print('successful episodes: {:d} - {:.4f}%'.format(successes, successes/episodes*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot History of Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=[10,5])\n",
    "p = pd.Series(position)\n",
    "ma = p.rolling(10).mean()\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(ma)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Car Final Position')\n",
    "plt.savefig('Final Position.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X = np.random.uniform(-1.2, 0.6, 10000)\n",
    "Y = np.random.uniform(-0.07, 0.07, 10000)\n",
    "Z = []\n",
    "for i in range(len(X)):\n",
    "    # Get the best action for the distribution of states\n",
    "    #_, temp = torch.max(q_func(np.array([X[i],Y[i]])), dim =-1)\n",
    "    temp = tf.math.argmax(q_func.forward(np.array([X[i],Y[i]])))\n",
    "    \n",
    "    z = temp.numpy()\n",
    "    Z.append(z)\n",
    "Z = pd.Series(Z)\n",
    "colors = {0:'blue',1:'lime',2:'red'}\n",
    "colors = Z.apply(lambda x:colors[x])\n",
    "labels = ['Left','Right','Nothing']\n",
    "\n",
    "fig = plt.figure(3, figsize=[7,7])\n",
    "ax = fig.gca()\n",
    "plt.set_cmap('brg')\n",
    "surf = ax.scatter(X,Y, c=Z)\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Velocity')\n",
    "ax.set_title('Q-Function')\n",
    "recs = []\n",
    "for i in range(0,env.action_space.n):\n",
    "     recs.append(mpatches.Rectangle((0,0),1,1,fc=sorted(colors.unique())[i]))\n",
    "plt.legend(recs,labels,loc=4,ncol=env.action_space.n)\n",
    "fig.savefig('Q_function.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "while not done:\n",
    "    Q = q_func.forward(state)\n",
    "    action = tf.math.argmax(Q)\n",
    "    action = action.numpy()\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    state = next_state\n",
    "    step += 1\n",
    "    print(\"Step: {}\".format(step))\n",
    "    env.render()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
