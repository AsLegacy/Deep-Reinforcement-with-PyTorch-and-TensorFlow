{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding best policy by brute force\n",
    "On this example we will do the simplest way of finding a policy for the cart pole problem, by brute force. This problem is simple because the state space and action space is small.\n",
    "\n",
    "#### State Space (1x4 vector)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "#### Action space (1x2 vector)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right\n",
    "\n",
    "#### Search Space\n",
    "On this problem the search space is 8 because the action space has 2 elements and the state space 4 elements:\n",
    "$$len(\\text{S}_{\\text{1x4}}) . len(\\text{A}_\\text{1x2})$$\n",
    "\n",
    "#### Episode Termination\n",
    "* Pole Angle is more than ±12°\n",
    "* Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "* Episode length is greater than 200\n",
    "\n",
    "### References\n",
    "* https://github.com/openai/gym/wiki/CartPole-v0\n",
    "* https://medium.com/@m.alzantot/deep-reinforcement-learning-demystified-episode-0-2198c05a6124\n",
    "* https://ray.readthedocs.io/en/latest/index.html\n",
    "* https://bair.berkeley.edu/blog/2018/01/09/ray/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Only log errors\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "# Policies to generate\n",
    "n_policy = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "On this case our parametrized policy will receive our state $S_\\text{1x4}$ and do a dot product with it's internal set of parameters $\\theta_{1x4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random 1x4 vector for the policy parameters\n",
    "def gen_random_policy_params():\n",
    "    return (np.random.uniform(-1,1, size=4).astype(np.float32))\n",
    "\n",
    "# Evaluate a policy given it's parameters and some state\n",
    "def policy(policy_params, state):\n",
    "    # It's basically a linear model\n",
    "    if np.dot(policy_params, state) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a list of possilbe policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated list of 50000 policies parameters\n"
     ]
    }
   ],
   "source": [
    "policy_params_list = [gen_random_policy_params() for _ in range(n_policy)]\n",
    "print('Generated list of %d policies parameters' % len(policy_params_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Policy\n",
    "This function basically run a policy during "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_episode(policy_params, render=False):\n",
    "    # Crete environment\n",
    "    env = gym.make('CartPole-v0')\n",
    "    state = env.reset()\n",
    "    # Total rewards from eposide\n",
    "    total_reward_episode = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        selected_action = policy(policy_params, state)\n",
    "        state, reward, done, _ = env.step(selected_action)\n",
    "        total_reward_episode += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardo_a/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 s, sys: 56.4 s, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate the policies on in the environment\n",
    "scores_list = [evaluate_policy_episode(p) for p in policy_params_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Best Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best policy score = 200\n",
      "Found best policy with params: [-0.6190245   0.4777126   0.01823577  0.9735814 ]\n",
      "Total rewards on best policy: 191.0\n",
      "Something went wrong\n"
     ]
    }
   ],
   "source": [
    "# Select the best policy from the score list.\n",
    "best_score = max(scores_list)\n",
    "print('Best policy score = %d' % best_score)\n",
    "\n",
    "best_policy_params= policy_params_list[np.argmax(scores_list)]\n",
    "print('Found best policy with params:', best_policy_params)\n",
    "\n",
    "# Run best policy [0.32896566 0.56930596 0.8278743  0.43927363]\n",
    "total_rewards_best_policy = evaluate_policy_episode(best_policy_params, render=True);\n",
    "print('Total rewards on best policy:', total_rewards_best_policy)\n",
    "\n",
    "if total_rewards_best_policy != best_score:\n",
    "    print('Something went wrong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Brute Force\n",
    "One of the advantages of the brute force method is that we can easily make things in parallel. For this example we will use the Ray framework to evalulate the list of policies in parallel.\n",
    "\n",
    "```python\n",
    "import time\n",
    "import ray\n",
    "ray.init()\n",
    "\n",
    "def f1():\n",
    "    time.sleep(1)\n",
    "\n",
    "@ray.remote\n",
    "def f2():\n",
    "    time.sleep(1)\n",
    "\n",
    "# The following takes ten seconds.\n",
    "[f1() for _ in range(10)]\n",
    "\n",
    "# The following takes one second (assuming the system has at least ten CPUs).\n",
    "ray.get([f2.remote() for _ in range(10)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and instantiate the distributed computing server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-23 16:14:26,375\tWARNING worker.py:1354 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-02-23 16:14:26,377\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-23_16-14-26_45532/logs.\n",
      "2019-02-23 16:14:26,486\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:28055 to respond...\n",
      "2019-02-23 16:14:26,607\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:63329 to respond...\n",
      "2019-02-23 16:14:26,613\tINFO services.py:798 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-02-23 16:14:26,640\tINFO services.py:1360 -- Starting the Plasma object store with 6.871947672999999 GB memory using /tmp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=f44241cbae037a8d53c5ccd8699478c249f4d3973ab7855e\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': None,\n",
       " 'redis_address': '192.168.120.200:28055',\n",
       " 'object_store_address': '/tmp/ray/session_2019-02-23_16-14-26_45532/sockets/plasma_store',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=f44241cbae037a8d53c5ccd8699478c249f4d3973ab7855e',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-02-23_16-14-26_45532/sockets/raylet'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def evaluate_policy_episode_par(policy_params):\n",
    "    # Crete environment\n",
    "    env = gym.make('CartPole-v0')\n",
    "    state = env.reset()\n",
    "    # Total rewards from eposide\n",
    "    total_reward_episode = 0\n",
    "    while True:\n",
    "        selected_action = policy(policy_params, state)\n",
    "        state, reward, done, _ = env.step(selected_action)\n",
    "        total_reward_episode += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Policies in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.65 s, sys: 900 ms, total: 8.55 s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_list_par = ray.get([evaluate_policy_episode_par.remote(p) for p in policy_params_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Best Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best policy score = 200\n",
      "Found best policy with params: [-0.38530764  0.32140714  0.53714424  0.91360974]\n",
      "Total rewards on best policy: 200.0\n"
     ]
    }
   ],
   "source": [
    "# Select the best policy from the score list.\n",
    "best_score = max(scores_list)\n",
    "print('Best policy score = %d' % best_score)\n",
    "\n",
    "best_policy_params= policy_params_list[np.argmax(scores_list_par)]\n",
    "print('Found best policy with params:', best_policy_params)\n",
    "\n",
    "# Run best policy [0.32896566 0.56930596 0.8278743  0.43927363]\n",
    "total_rewards_best_policy = evaluate_policy_episode(best_policy_params, render=True);\n",
    "print('Total rewards on best policy:', total_rewards_best_policy)\n",
    "\n",
    "if total_rewards_best_policy != best_score:\n",
    "    print('Something went wrong')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
